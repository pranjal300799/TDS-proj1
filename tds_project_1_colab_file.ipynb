{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Define the list to store users from Delhi\n",
        "users_in_delhi = []\n",
        "\n",
        "# Read the CSV file with UTF-8 encoding\n",
        "with open('users.csv', 'r', encoding='utf-8') as file:\n",
        "    reader = csv.DictReader(file)\n",
        "    for row in reader:\n",
        "        location = row['location'].strip().lower()\n",
        "        # Check if the user is from Delhi\n",
        "        if 'delhi' in location:\n",
        "            users_in_delhi.append({\n",
        "                'login': row['login'],\n",
        "                'followers': int(row['followers'])\n",
        "            })\n",
        "\n",
        "# Sort users based on followers in descending order\n",
        "top_users = sorted(users_in_delhi, key=lambda x: x['followers'], reverse=True)\n",
        "\n",
        "# Extract the top 5 user logins\n",
        "top_5_logins = [user['login'] for user in top_users[:5]]\n",
        "\n",
        "# Print the result as a comma-separated list\n",
        "print(','.join(top_5_logins))\n"
      ],
      "metadata": {
        "id": "ciaWrowWnDsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "# Define the list to store users from Delhi\n",
        "users_in_delhi = []\n",
        "\n",
        "# Read the CSV file with UTF-8 encoding\n",
        "with open('users.csv', 'r', encoding='utf-8') as file:\n",
        "    reader = csv.DictReader(file)\n",
        "    for row in reader:\n",
        "        location = row['location'].strip().lower()\n",
        "        # Check if the user is from Delhi\n",
        "        if 'delhi' in location:\n",
        "            users_in_delhi.append({\n",
        "                'login': row['login'],\n",
        "                'created_at': datetime.strptime(row['created_at'], '%Y-%m-%dT%H:%M:%SZ')\n",
        "            })\n",
        "\n",
        "# Sort users based on created_at in ascending order\n",
        "sorted_users = sorted(users_in_delhi, key=lambda x: x['created_at'])\n",
        "\n",
        "# Extract the top 5 user logins\n",
        "top_5_earliest_logins = [user['login'] for user in sorted_users[:5]]\n",
        "\n",
        "# Print the result as a comma-separated list\n",
        "print(','.join(top_5_earliest_logins))\n"
      ],
      "metadata": {
        "id": "vPUdSZoknFW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from collections import Counter\n",
        "\n",
        "# Define the list to store license names\n",
        "licenses = []\n",
        "\n",
        "# Read the CSV file with UTF-8 encoding\n",
        "with open('repositories.csv', 'r', encoding='utf-8') as file:\n",
        "    reader = csv.DictReader(file)\n",
        "    for row in reader:\n",
        "        # Check if the license_name field is present and not empty\n",
        "        license_name = row.get('license_name', '').strip()\n",
        "        if license_name:\n",
        "            licenses.append(license_name)\n",
        "\n",
        "# Count the occurrence of each license\n",
        "license_counts = Counter(licenses)\n",
        "\n",
        "# Get the 3 most common licenses\n",
        "top_3_licenses = [license for license, count in license_counts.most_common(3)]\n",
        "\n",
        "# Print the result as a comma-separated list\n",
        "print(','.join(top_3_licenses))\n"
      ],
      "metadata": {
        "id": "n47RPhZSnKeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from collections import Counter\n",
        "\n",
        "# Define the list to store company names\n",
        "companies = []\n",
        "\n",
        "# Read the CSV file with UTF-8 encoding\n",
        "with open('users.csv', 'r', encoding='utf-8') as file:\n",
        "    reader = csv.DictReader(file)\n",
        "    for row in reader:\n",
        "        # Get and clean up the company field (ignore empty values)\n",
        "        company = row.get('company', '').strip()\n",
        "        if company:\n",
        "            companies.append(company)\n",
        "\n",
        "# Count the occurrence of each company\n",
        "company_counts = Counter(companies)\n",
        "\n",
        "# Find the most common company\n",
        "most_common_company = company_counts.most_common(1)\n",
        "\n",
        "# Print the result\n",
        "if most_common_company:\n",
        "    print(most_common_company[0][0])\n",
        "else:\n",
        "    print(\"No company data found.\")\n"
      ],
      "metadata": {
        "id": "abkU3HtfnQqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from collections import Counter\n",
        "\n",
        "# Define the list to store programming languages\n",
        "languages = []\n",
        "\n",
        "# Read the CSV file with UTF-8 encoding\n",
        "with open('repositories.csv', 'r', encoding='utf-8') as file:\n",
        "    reader = csv.DictReader(file)\n",
        "    for row in reader:\n",
        "        # Get and clean up the language field (ignore empty values)\n",
        "        language = row.get('language', '').strip()\n",
        "        if language:\n",
        "            languages.append(language)\n",
        "\n",
        "# Count the occurrence of each language\n",
        "language_counts = Counter(languages)\n",
        "\n",
        "# Find the most common language\n",
        "most_common_language = language_counts.most_common(1)\n",
        "\n",
        "# Print the result\n",
        "if most_common_language:\n",
        "    print(most_common_language[0][0])\n",
        "else:\n",
        "    print(\"No language data found.\")\n"
      ],
      "metadata": {
        "id": "etiAWBSYnZiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "\n",
        "# Define the list to store programming languages\n",
        "languages = []\n",
        "\n",
        "# Read the CSV file with UTF-8 encoding\n",
        "with open('repositories.csv', 'r', encoding='utf-8') as file:\n",
        "    reader = csv.DictReader(file)\n",
        "\n",
        "    # Iterate through the rows in the CSV\n",
        "    for row in reader:\n",
        "        # Parse the created_at field\n",
        "        created_at = row.get('created_at', '').strip()\n",
        "\n",
        "        # Convert the date string to a datetime object\n",
        "        if created_at:\n",
        "            user_join_date = datetime.strptime(created_at, \"%Y-%m-%dT%H:%M:%SZ\")\n",
        "\n",
        "            # Check if the user joined after 2020\n",
        "            if user_join_date.year > 2020:\n",
        "                # Get the language field and clean it up\n",
        "                language = row.get('language', '').strip()\n",
        "                if language:\n",
        "                    languages.append(language)\n",
        "\n",
        "# Count the occurrence of each language\n",
        "language_counts = Counter(languages)\n",
        "\n",
        "# Find the two most common languages\n",
        "most_common_languages = language_counts.most_common(2)\n",
        "\n",
        "# Print the second most common language\n",
        "if len(most_common_languages) >= 2:\n",
        "    print(most_common_languages[1][0])  # Second most common language\n",
        "else:\n",
        "    print(\"Not enough language data found.\")\n"
      ],
      "metadata": {
        "id": "zSzMyQfnndvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from collections import defaultdict\n",
        "\n",
        "# Define a dictionary to store total stars and repository count per language\n",
        "language_stats = defaultdict(lambda: {'stars': 0, 'repos': 0})\n",
        "\n",
        "# Read the CSV file with UTF-8 encoding\n",
        "with open('repositories.csv', 'r', encoding='utf-8') as file:\n",
        "    reader = csv.DictReader(file)\n",
        "\n",
        "    for row in reader:\n",
        "        # Get the language and stargazers_count field\n",
        "        language = row.get('language', '').strip()\n",
        "        stars = row.get('stargazers_count', '0').strip()\n",
        "\n",
        "        # Only process if language and stars are available\n",
        "        if language and stars.isdigit():\n",
        "            language_stats[language]['stars'] += int(stars)\n",
        "            language_stats[language]['repos'] += 1\n",
        "\n",
        "# Calculate average stars for each language\n",
        "average_stars_per_language = {\n",
        "    language: stats['stars'] / stats['repos']\n",
        "    for language, stats in language_stats.items()\n",
        "    if stats['repos'] > 0\n",
        "}\n",
        "\n",
        "# Find the language with the highest average stars\n",
        "if average_stars_per_language:\n",
        "    most_popular_language = max(average_stars_per_language, key=average_stars_per_language.get)\n",
        "    print(most_popular_language)\n",
        "else:\n",
        "    print(\"No language data found.\")\n"
      ],
      "metadata": {
        "id": "GviMte_oneb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Define a list to store users and their leader strength\n",
        "leader_strengths = []\n",
        "\n",
        "# Read the CSV file with UTF-8 encoding\n",
        "with open('users.csv', 'r', encoding='utf-8') as file:\n",
        "    reader = csv.DictReader(file)\n",
        "\n",
        "    for row in reader:\n",
        "        # Get followers and following counts\n",
        "        followers = int(row.get('followers', 0).strip())\n",
        "        following = int(row.get('following', 0).strip())\n",
        "\n",
        "        # Calculate leader strength\n",
        "        leader_strength = followers / (1 + following)\n",
        "\n",
        "        # Store the user's login and their leader strength\n",
        "        leader_strengths.append((row.get('login', ''), leader_strength))\n",
        "\n",
        "# Sort users by leader strength in descending order\n",
        "leader_strengths.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Get the top 5 users\n",
        "top_5_leaders = [login for login, strength in leader_strengths[:5]]\n",
        "\n",
        "# Print the result as a comma-separated list\n",
        "print(','.join(top_5_leaders))\n"
      ],
      "metadata": {
        "id": "gbtQeSeoniQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "# Lists to store the followers and public repos of users from Delhi\n",
        "followers = []\n",
        "public_repos = []\n",
        "\n",
        "# Open the users.csv file and read data\n",
        "with open('users.csv', 'r', encoding='utf-8') as file:\n",
        "    reader = csv.DictReader(file)\n",
        "\n",
        "    for row in reader:\n",
        "        # Filter for users in Delhi\n",
        "        location = row.get('location', '').strip().lower()\n",
        "        if \"delhi\" in location:\n",
        "            # Get followers and public repositories values\n",
        "            try:\n",
        "                followers_count = int(row['followers'])\n",
        "                public_repos_count = int(row['public_repos'])\n",
        "\n",
        "                # Append the valid values to the lists\n",
        "                followers.append(followers_count)\n",
        "                public_repos.append(public_repos_count)\n",
        "            except ValueError:\n",
        "                # Skip rows with invalid numerical values\n",
        "                continue\n",
        "\n",
        "# Ensure there is data to compute correlation\n",
        "if len(followers) > 1 and len(public_repos) > 1:\n",
        "    # Compute Pearson correlation coefficient\n",
        "    correlation_matrix = np.corrcoef(followers, public_repos)\n",
        "    correlation = correlation_matrix[0, 1]\n",
        "    # Output correlation rounded to 3 decimal places\n",
        "    print(f\"{correlation:.3f}\")\n",
        "else:\n",
        "    print(\"Insufficient data for correlation calculation.\")\n"
      ],
      "metadata": {
        "id": "62aIHwOxnoTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "# Lists to store the followers and public repos of users from Delhi\n",
        "followers = []\n",
        "public_repos = []\n",
        "\n",
        "# Open the users.csv file and read data\n",
        "with open('users.csv', 'r', encoding='utf-8') as file:\n",
        "    reader = csv.DictReader(file)\n",
        "\n",
        "    for row in reader:\n",
        "        # Filter for users in Delhi\n",
        "        location = row.get('location', '').strip().lower()\n",
        "        if \"delhi\" in location:\n",
        "            # Get followers and public repositories values\n",
        "            try:\n",
        "                followers_count = int(row['followers'])\n",
        "                public_repos_count = int(row['public_repos'])\n",
        "\n",
        "                # Append the valid values to the lists\n",
        "                followers.append(followers_count)\n",
        "                public_repos.append(public_repos_count)\n",
        "            except ValueError:\n",
        "                # Skip rows with invalid numerical values\n",
        "                continue\n",
        "\n",
        "# Ensure there is data for regression\n",
        "if len(followers) > 1 and len(public_repos) > 1:\n",
        "    # Perform linear regression: followers ~ public_repos\n",
        "    slope, intercept = np.polyfit(public_repos, followers, 1)\n",
        "\n",
        "    # Output the slope rounded to 3 decimal places\n",
        "    print(f\"{slope:.3f}\")\n",
        "else:\n",
        "    print(\"Insufficient data for regression.\")\n"
      ],
      "metadata": {
        "id": "kECsIiWZnpC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Load the CSV file\n",
        "csv_file = 'repositories.csv'  # Replace with the correct path\n",
        "\n",
        "# Load the CSV into a DataFrame\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# Convert 'has_projects' and 'has_wiki' to boolean if necessary\n",
        "df['has_projects'] = df['has_projects'].astype(bool)\n",
        "df['has_wiki'] = df['has_wiki'].astype(bool)\n",
        "\n",
        "# Create a contingency table\n",
        "contingency_table = pd.crosstab(df['has_projects'], df['has_wiki'])\n",
        "\n",
        "# Perform Chi-Square test\n",
        "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "print(f\"Chi-Square Statistic: {chi2}\")\n",
        "print(f\"P-value: {p}\")\n"
      ],
      "metadata": {
        "id": "Onjqh5QgntcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def analyze_following_difference(users_csv_path='users.csv'):\n",
        "    # Read the data\n",
        "    df = pd.read_csv(users_csv_path)\n",
        "\n",
        "    # Calculate average following for hireable users\n",
        "    hireable_following = df[df['hireable'] == True]['following'].mean()\n",
        "\n",
        "    # Calculate average following for non-hireable users\n",
        "    non_hireable_following = df[df['hireable'] != True]['following'].mean()\n",
        "\n",
        "    # Calculate the difference rounded to 3 decimal places\n",
        "    difference = round(hireable_following - non_hireable_following, 3)\n",
        "\n",
        "    # Print debug information\n",
        "    print(f\"Number of hireable users: {len(df[df['hireable'] == True])}\")\n",
        "    print(f\"Number of non-hireable users: {len(df[df['hireable'] != True])}\")\n",
        "    print(f\"Average following for hireable users: {hireable_following:.3f}\")\n",
        "    print(f\"Average following for non-hireable users: {non_hireable_following:.3f}\")\n",
        "\n",
        "    return difference\n",
        "\n",
        "# Calculate the difference\n",
        "result = analyze_following_difference()\n",
        "print(f\"\\nDifference in average following: {result:.3f}\")"
      ],
      "metadata": {
        "id": "TrEoZEECnv5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "def analyze_bio_followers_correlation(users_csv_path='users.csv'):\n",
        "    # Read the data\n",
        "    df = pd.read_csv(users_csv_path)\n",
        "\n",
        "    # Filter out rows without bios\n",
        "    df = df[df['bio'].notna() & (df['bio'] != '')]\n",
        "\n",
        "    # Calculate bio length in Unicode characters\n",
        "    df['bio_length'] = df['bio'].str.len()\n",
        "\n",
        "    # Prepare data for regression\n",
        "    X = df['bio_length'].values.reshape(-1, 1)\n",
        "    y = df['followers'].values\n",
        "\n",
        "    # Perform linear regression\n",
        "    model = LinearRegression()\n",
        "    model.fit(X, y)\n",
        "\n",
        "    # Get the slope rounded to 3 decimal places\n",
        "    slope = round(model.coef_[0], 3)\n",
        "\n",
        "    # Print debug information\n",
        "    print(f\"Number of users with bios: {len(df)}\")\n",
        "    print(f\"Bio length range: {df['bio_length'].min()} to {df['bio_length'].max()}\")\n",
        "    print(f\"Followers range: {df['followers'].min()} to {df['followers'].max()}\")\n",
        "    print(f\"R-squared: {model.score(X, y):.3f}\")\n",
        "\n",
        "    return slope\n",
        "\n",
        "# Calculate the regression slope\n",
        "result = analyze_bio_followers_correlation()\n",
        "print(f\"\\nRegression slope: {result:.3f}\")"
      ],
      "metadata": {
        "id": "YOjpX4LYnwrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "\n",
        "# Counter to store the number of repositories created by each user on weekends\n",
        "weekend_repo_counts = Counter()\n",
        "\n",
        "# Open the repositories.csv file and read data\n",
        "with open('repositories.csv', 'r', encoding='utf-8') as file:\n",
        "    reader = csv.DictReader(file)\n",
        "\n",
        "    for row in reader:\n",
        "        created_at = row.get('created_at', '')\n",
        "        if created_at:\n",
        "            # Convert created_at string to a datetime object\n",
        "            created_date = datetime.fromisoformat(created_at[:-1])  # Remove 'Z' and convert\n",
        "\n",
        "            # Check if the day is Saturday (5) or Sunday (6)\n",
        "            if created_date.weekday() in [5, 6]:\n",
        "                user_login = row['login']\n",
        "                weekend_repo_counts[user_login] += 1  # Increment the count for the user\n",
        "\n",
        "# Get the top 5 users who created the most repositories on weekends\n",
        "top_users = weekend_repo_counts.most_common(5)\n",
        "\n",
        "# Extract the logins of the top users\n",
        "top_logins = [user[0] for user in top_users]\n",
        "\n",
        "# Output the top users' logins as a comma-separated string\n",
        "print(','.join(top_logins))\n"
      ],
      "metadata": {
        "id": "fWH6E02Unz3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def analyze_email_sharing(users_csv_path='users.csv'):\n",
        "    # Read the complete CSV file\n",
        "    df = pd.read_csv(users_csv_path)\n",
        "\n",
        "    # Convert email column to boolean (True if email exists, False if NaN or empty)\n",
        "    df['has_email'] = df['email'].notna() & (df['email'] != '')\n",
        "\n",
        "    # Calculate for hireable users\n",
        "    hireable_mask = df['hireable'] == True\n",
        "    if hireable_mask.any():\n",
        "        hireable_email_fraction = df[hireable_mask]['has_email'].mean()\n",
        "    else:\n",
        "        hireable_email_fraction = 0\n",
        "\n",
        "    # Calculate for non-hireable users\n",
        "    non_hireable_mask = df['hireable'] != True\n",
        "    if non_hireable_mask.any():\n",
        "        non_hireable_email_fraction = df[non_hireable_mask]['has_email'].mean()\n",
        "    else:\n",
        "        non_hireable_email_fraction = 0\n",
        "\n",
        "    # Calculate difference and round to 3 decimal places\n",
        "    difference = round(hireable_email_fraction - non_hireable_email_fraction, 3)\n",
        "\n",
        "    # Print debug information\n",
        "    print(f\"Total users: {len(df)}\")\n",
        "    print(f\"Hireable users with email: {df[hireable_mask]['has_email'].sum()}/{hireable_mask.sum()}\")\n",
        "    print(f\"Non-hireable users with email: {df[non_hireable_mask]['has_email'].sum()}/{non_hireable_mask.sum()}\")\n",
        "    print(f\"Hireable fraction: {hireable_email_fraction:.3f}\")\n",
        "    print(f\"Non-hireable fraction: {non_hireable_email_fraction:.3f}\")\n",
        "\n",
        "    return difference\n",
        "\n",
        "# Read and analyze the complete dataset\n",
        "result = analyze_email_sharing()\n",
        "print(f\"\\nFinal result: {result:.3f}\")"
      ],
      "metadata": {
        "id": "AQkUvzran3Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from collections import Counter\n",
        "\n",
        "# Counter to store surname frequencies\n",
        "surname_counter = Counter()\n",
        "\n",
        "# Open the users.csv file and read data\n",
        "with open('users.csv', 'r', encoding='utf-8') as file:\n",
        "    reader = csv.DictReader(file)\n",
        "\n",
        "    for row in reader:\n",
        "        name = row.get('name', '').strip()\n",
        "        if name:  # Ignore missing names\n",
        "            # Split the name by whitespace and get the last word as the surname\n",
        "            surname = name.split()[-1]\n",
        "            surname_counter[surname] += 1\n",
        "\n",
        "# Find the maximum frequency of surnames\n",
        "if surname_counter:\n",
        "    max_count = max(surname_counter.values())\n",
        "    # Get all surnames with the maximum frequency\n",
        "    most_common_surnames = [surname for surname, count in surname_counter.items() if count == max_count]\n",
        "    # Sort surnames alphabetically\n",
        "    most_common_surnames.sort()\n",
        "    # Output the result\n",
        "    print(f\"{', '.join(most_common_surnames)}: {max_count}\")\n",
        "else:\n",
        "    print(\"No names found.\")\n"
      ],
      "metadata": {
        "id": "-Z-1XtWmn51a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}